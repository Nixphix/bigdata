# will demonstrate how to import tables in hive to hdfs
# prequisite: some table must be persent in hive store

# ckeck if the sourece table is present and has data
hive
show databases;
show tables;
select * from departments;

# launch pyspark with yarn as master 
pyspark -master yarn

# import Hive Context form pyspark.sql, create hive context by passing spark ctxt
from pyspark.sql import HiveContext
hc = HiveContext(sc)

dept = hc.sql("select * from departments")   # query a table 
for row in dept.collect():
    print row.department_name, row.department_id

ctas = hc.sql("create table dept as select * from departments") # no data will be returned
ctas

ctas = hc.sql("select * from dept")   # query the created table 
ctas # prints the collection type 
for row in ctas.collect():
    print row.department_name, row.department_id

droptbl = hc.sql("drop table dept")

ctas = hc.sql("select * from dept")   # table dept not found error will be thrown

# create a table employees
ct = hc.sql("create table if not exists employees(firstName string, lastName string)")

# list the tables in hive store # hc.sql("show tables").collect()
[row.tableName.encode('ascii') for row in hc.sql("show tables").collect()]

# description of employees table in hive store # hc.sql("desc employees").collect()
[(row.col_name.encode('ascii')+" "+row.data_type.encode('ascii')) for row in hc.sql("desc employees").collect()]
